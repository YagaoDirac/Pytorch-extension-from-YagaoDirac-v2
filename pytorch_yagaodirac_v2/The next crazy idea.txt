就是当时的旋转神经网络的事情。
当时的做法是用一个n*n的方阵来表示旋转，然后每次step之后，对这个旋转矩阵进行规范化，让它再回到旋转矩阵。
但是旋转矩阵本身，构造容易，但是修复难。最后就没做成。
如果拆成2个坐标轴组成的旋转行为，就会有先后啊，gimble lock（中文反正是死锁），这些问题，加上真的很高维的情况下，要覆盖一整遍，复杂度是n平方。具体的说是 n*(n-1)/2，所以很不靠谱。
然后刚刚突然在想，其实不用真的完全覆盖一遍，比如在4维，第一层做xy，然后zw，2个旋转，第二层做xz，yw，这样分，然后重复几遍，是不是就完事了，本身深度学习也要重复。
这种矩阵的好处就是特别容易修复，每一个矩阵都可以分解成很多个2*2的小矩阵，这种小矩阵是随便都可以手写一个方法去修复的。
最后还是老问题，非线性怎么引入，之前的类似relu的做法，感觉有点粗糙，不过也可以作为起点。
总结下来就是，虽然我很清晰的知道这个东西在神经网络宽度大了以后是完全没有用的，但是就是会忍不住去想。典型的和ai互相折磨。