我所知道的神经网络的细节

作者 牙膏Dirac(yagaodirac)



背景。
2025年，神经网络领域依然没有特别系统的针对理论的归纳。
我刚好比较擅长。于是就写一下。
所有内容都是我本人的经验。主要是在比较底层的部分。
我不太懂神经网络结构设计，感觉没什么用，就没花时间。不太懂提示，因为transformer给我的工作没有帮助。不太懂机器学习，因为用不上。

总之，尽量短。看不懂的话请结合网络资料。



# 第一章 信息量/信息流动/模型宽度/感受野/信息纯度与提纯

这个事情最早是感受野，这个概念其实是在conv里面提出的。
后来conv中间夹注意力层，发现性能好像不错，解释就是注意力层的感受野是全部，而不是一个逐渐变大的锥形。
信息量其实是信息论里面的概念。
信息流动最早是解释gan和auto encoder的关系。gan每一个轮次只吸收1bit的信息，而且在非对称的情况下，平均信息量甚至还低于1bit，而auto encoder一类的生成模型，每一个轮次吸收的信息量就非常巨大了。
https://x.com/evilmathkid/status/2001689479476879448
这个里面提到的方法，不说具体实现，单说让模型输出一次输入，这个做法本身，就是在提高单一轮次的反向传播的信息总量。这个方法应该要尽量用到所有有可能的地方。

其实前向传播也有类似的概念。后面动力学的部分会说一个叫“信息消失”的问题，是我发明的名词，但是内容是从来都存在的。这个地方只讲模型宽度。
线性代数上有一个概念叫rank，中文叫zhi，4声，我打不出来这个字。模型的宽度越大，通常来说能透过的信息量就越大，很好理解。注意一个陷阱，就是如果你用低秩矩阵分解，low rank matrix decomposition/factorization，中间那个地方的宽度是会降低的，要注意。另外就是，可以根据每一个通路上的weight matrix的rank来大致评估这一条通路的实际透过的信息量。不过考虑到计算是有误差的，和数学上的那种rank的定义肯定会有区别，比如两个行几乎一样，这种怎么处理，暂时还不知道。不过如果有闲心，可以搭一个低秩矩阵分解，用神经网络的方法训一下，看看rank到多少误差就压不下去了，勉强作为一个测量方法吧。

关于每一个轮次输入的信息量，除了前面提到的gan，还有一些可以稍微分析一下的。
遗传算法和基于噪音的训练，两个有一些相似性。
（我简单解释一下基于噪音的训练，就是加一个噪音，然后看这个噪音对模型的影响，如果是正面的就保留，否则就舍去，或者反向保留）
最核心的相似性就是他们每一次只吸收1bit的信息，和gan很相似。当然，因为毕竟吸收的是一个浮点数，所以你也可以认为不止1bit，但是无论如何也不会很多，双精度拉满了也不到64bit。
但是遗传算法，gan，这一类的方法在模型本身很小，任务很小，这种对总信息量不大的场景还是可以用的，甚至可能还有优势。
题外话。和做遗传算法（无论任何分支）的人聊天，可以问一句，“这个方法大概可以用在多大的任务上？”一个字都不用改。

<2026 jan 7 补充>
突然发现我没写信息提纯的问题。。
先说一个比喻，大多数时候，提纯技术会限制人类能得到的物质的最大纯度。地球上最纯的物质是单晶硅，9到10个9，所有的相关的制品都要收这个的限制。化学实验里面，如果要合成某种化合物，那么实际上能得到的最极端的化合物，或者混合物，也一定不会比最强的提纯技术能得到的“纯”物质更纯。
信息也是一样的，人类能利用到的最纯的信息，不会比能提取到的最纯的信息更纯，因为应用信息的过程是一个混合的过程，信息的纯度至少不可能增加。
现在已知的提纯信息的方法基本就是ai和相关的技术了。
机器学习，统计，一类的方法，适应能力比较弱，但是一旦遇到可能性空间相同的情况，能很好的提纯信息。
深度学习适应能力非常强，但是提纯信息的效果非常有限。
强化学习本身和信息提纯无关，有关的是里面的神经网络，所以见上一行。
我现在已知的方法里面还有一个是逻辑学习（见这个repo）（我发明滴~），它的信息提纯能力应该是接近机器学习的，但是适应能力非常强。这是我现在已知在非特定的情况下，最能提纯信息的方法。
还有一个数字神经网络（或许今年能做出来一个能用的版本吧），如果能做出来，应该也是很强的。

检验方法基本上就是用已知的某种混合信息的方法，把一些比较纯的信息混合起来，看特定的方法是否能反过来找到很纯的信息。最好的原料是在逻辑层面上相互正交的信息，这种信息在文本上可能比较不好找，尤其是要保证信息互相之间正交，很容易出问题。最简单，可操作，低成本的方法，就是小学数学，让一个模型或者什么东西去学加法，乘法。
在离散（二进制）和连续表达上，信息的提纯在形式上也有区别。在二进制的情况下，利用一个输入的一个位在特定输入下，和结果之间的相关程度，可以进行一定的外延。而实际上二进制的逻辑，除了训练数据里面直接给出的，其他的都是外延。二进制和实数的一个根本区别就在，实数比较容易训练数据覆盖了的“区域”做插值，而很难在没有“覆盖”的地方做外延。
现在遇到过的问题是，如果用二进制表达，那么这个模型必须要能处理xor逻辑，不然加法里面一定会有xor，这个地方可能就会引入一个非常强的缺陷，而实际问题当中可能没有这么集中的xor，这个细节会造成评估的误差。
所以可能回到第一性原理，研发针对二进制的方法，才是下一阶段最重要的事情。

至于是否有办法从信息量上直接定量的得到提纯的检验标准，暂时毫无思路，包括在二进制下和实数下的具体做法，还请各位大佬指点。
</2026 jan 7 补充>









# 第二章 训练动力学，信息消失，正向传播的稳定性，以及我对误差传播的公式的一点质疑

本人现在的主要研究方向就是这个。
简单的说，我希望每一次的计算都尽量让模型往其中一个足够优的状态上移动尽量大，但是不会造成破坏。
在不考虑残差链接的情况下，模型不同部位的层的更新强度是不同的，可以自己去推公式。当weight会让x边长的时候，wx的向量长度会大于x，加b的结果通常来说会变长一点点，不过可以忽略。反向传播和前向传播唯一的区别就是，不用加b。如果为了保证前向传播的参数不会变得非常巨大，通常会让它稍微减小一点点，也就是weight会根据维度来对里面的值进行缩放。pytorch给了一个很好的例子，里面使用的初始化是何凯明的。如果你新建一个torch.nn.Linear，设置不同的维度，就会明显的发现，维度大的，weight里面的数字会变小。He的初始化会让正向传播看上去比较正常，通常深度不超过15层的时候，输入值的变化还是能影响到输出值的，但是如果wx相比b特别小，那么输出值将会主要由模型参数来决定，或者说，主要由靠近输出的几个层的模型参数来决定，而且主要是最后一层的b。
我把这个现象叫做信息消失。取名字的时候参考了梯度消失。
信息消失的原因除了上面提到的，w太小，wx相比b太小，层数太多，还有一种情况，是训练过程当中形成的。当input无论如何变，label都不变的时候，模型可能会走向一种无视input的状态，例如在全连接层里面，让w接近0，用b来接近label，可以很容易用实验来复现。这种情况比较常见的是在强化学习里面，尤其是当reward function的逻辑不是很适合里面的神经网络的时候。
还是稍微说说梯度消失吧。
在我的体系里面，参数分几个大类。超参数是人工设定的，训练过程里面不变的，这里不讨论。在训练过程当中由训练数据和模型共同产生的，按我的习惯分3大类，前向传播当中的流动参数（代码里面就是forward函数里面的x），反向传播当中的流动参数（梯度），模型参数。3种参数都需要进行保护，不然都会出各种问题。
前面讨论了前向流动参数的消失问题，其实不一定要变成0才是消失，而是携带的信息没有了，或者无法利用了，就算消失。反向传播也会有这个问题，而且原理完全一样。另外就是参数爆炸问题，3种参数都有可能爆炸，但是今天不想讨论。
梯度消失可能是参数保护里面被讨论得最多的。前面提到了为什么梯度会消失，现在讲讲解决方案。
首先剧透一下，我代码里面给了解决方案了，文件叫parammo.py。读完这一段你会知道怎么使用。
先补充一个第一性原理。假设一个向量有100维，没有其他限制的情况下，线性自由度是100，或者说，可以用100个数字来表达信息（我不懂实数的信息量的定量，姑且只说线性自由度），那么如果把长度固定下来，只用方向表达信息，会损失1个线性自由度。在不考虑0向量一类的极端情况的条件下，这个说法是成立的。
那么如果反向传播的梯度（作为一个向量），是携带的信息在模型里面传播，那么扔掉向量的长度，只要不是只有1到2个维度，实际上是会保留下来大多数信息的，包括3维的向量，也会保留2/3的信息。当然你的模型不可能只有3的宽度。
通常来说，反向传播的公式就是输入的梯度的具体值，matmal上weight，顺着输入传出去，或者matmal上输入，积累到weight上。bias的情况比较简单，自己推。那么weight上积累的梯度的数量级，无论是按照矩阵里面的每一个元素作为评价依据，还是整个矩阵作为一个整体，数量级都是可以大概定义并且计算的。那么到底积累了多少，具体公式可能很不一样，但是总的来说，grad_in越靠近0，x越靠近0，w积累到的梯度就越靠近0。积累到的梯度最后是减（和加一个意思）在原来的数值上的，所以越靠近0，就越相当于没更新。
那么我感觉说到这儿，应该猜到我要说什么了。
如果让积梯度作为一个向量，按照长度尽量不变，而方向随意，进行保护，那么它有可能在反向传播的整个过程当中，既不消失，也不爆炸。如果你们读我的enhanced mlp那个文件，会发现，除了weight刚刚出来直接经过一次某个特殊的层，我还会在主梯度链上沿途设置这个层，保护沿主梯度链传播的梯度。
一个题外话，我没试过，但是基于正态分布的norm应该也可以，你们去试试？

神经网络可训练性在我的知识范围里面还有一个经验，暂时没有系统的验证或者第一性原理的论证，就是前向传播的稳定性，定义是在相邻的epoch上，相同的输入值让模型所有内部的前向传播行为从数值上尽量是连续变化的，至少不应该出现在可能的极端取值的两个极限上反复突变。
总的来说，神经网络的误差传播的做法，实际上是梯度最终积累到模型参数上，从而影响模型的前向传播的行为，这是一种很类似积分的行为。因为积分是低通滤波器，反向传播上的行为对正向传播的影响应该是在不同的epoch之间是连续的，整体性的，不应该引起突变。
违背这个经验的情景主要有dropout，raw batch norm，还有我之前尝试过一下的数字神经网络。
dropout的情况呢，你们看看它的定义基本就能猜到了。
batch norm现在很多种了，我说的是那种最最原始的版本。现在新的版本在mean和std上都是用的一个逐步变化的值，这种逐步变化就让整个层的行为符合了我说的，前向传播的稳定性。至于更新两个超参数的具体的公式嘛，其实思路可以开阔一点。
其实基本上所有的前向传播里面的自适应的行为都会导致精度明显很糟糕。
最后还有一个数字神经网络。这个是我取的名字。现在国际上至少还有2个组在做类似的工作，他们都发过论文，我没有发过。这个东西最大的问题是，它的前向传播的值只有2个，天然的也就是2个极端值，每次发生变化的时候都会发生突变。最终，在我手上，这东西都只能叠到8层。

<2026 jan 19 补充>
我对误差传播的公式的一点质疑
这个思路存在很久了，一直没用写代码去验证。
简单的说就是，我希望让最终的误差，映射到每一层，让每一层看到一个自己应该微调的方向。
传统的做法，也就是现在在用的误差传播的方法，是求偏导数，大概可以理解为，label减去预测，作为一个delta，每一个matrix的每一个element通过往一个方向调整，会对最终的预测有一个影响，通过求偏导数，得到这个关系。
说个简单一点的就是，z=0.5x+0.2y，那么两个偏导数分别是0.5和0.2。为了让z增大，那么x和y都应该增大。但是增大的幅度的关系，真的是0.5比0.2吗。其实这个比例并不说明任何问题。
假设z=0.5x+0.2y（和上面一样），单独改变x，要让z增加1，那么x应该增加2，如果单独改y，那么y应该增加5。假设x和y能承受的变化强度差不多，那么如果因为种种限制导致他们应该增加相同的量，那么这个量是1.3（1/0.7）左右。
但是无论如何，在这个例子里面，x和y都应该增大。
某种意义上来说，无论用任何方法，一个元素得到的更新的依据，可能只有符号。
但是依然还是有一个问题，到底应该是像现在用的求偏导数的方法，还是应该把delta映射回去。
为了说明映射到底是什么，重新说一个例子，pred = x*2，（*表示乘法），为了让pred变大1，x应该变大0.5，公式是，delta of x = delta of pred/2，可以看到，这个地方是除法，而不是偏导数里面的乘法。如果y=x*w，w=2，就是上面的这个例子。x得到的偏导数是2，而将1映射回x，得到的是0.5。
如果在线性代数里面，这个就要牵扯到方阵的求逆。
而求逆本身是一个很麻烦的事情，加上实际操作当中，当方阵很大的时候，基本不可能保证可逆。这个时候就要牵扯到方阵本身的保护问题。
另外，虽然直觉上来说，两个思路的公式是不同的，但是最终在参数上积累到的更新依据会不会有一定的相似性，无论是现在的误差传播里面的梯度，还是把梯度的绝对值扔掉只看符号，还是我说的映射回去的方法得到的某种信息，如果他们都可以训一个神经网络，那么他们应该存在某种等效性？
</2026 jan 19 补充>




#第三章 可能性空间。

这个事情其实是神经网络的一个重灾区，没有人提过，但是几乎所有人的所有项目都是受害者。
可能性空间的定义是，一个神经网络在不改变结构的情况下，单纯通过改变参数，可以给出的行为，或者说可以拟合的函数的集合。在允许一定的误差的情况下，还可以再大一点。实际使用当中肯定多少是允许误差的，但是这个细节不重要。
先说现状。现状就一句话，没有依据，直接用。
其实所有人都可以做的一个实验，输入是2个随机数，范围随意，比如是x1和x2，输出是y=x1*x2，随便搭一个mlp直接训就行了。不需要我说任何多余的话。结论很清晰，模型要大到很抽象，精度才能足够高。原因也很直观，如果这个神经网络里面总共就只有wx+b，和激活函数，或者就算你还加了一点什么norm一类的，无所谓，不影响，仔细看，没有什么地方能给出一个x1*x2。
但是如果稍微魔改一下，x=[x1,x2]，第一层里面改成return x.T.matmal(x)（转置可能写反了），这个直接就引入了x1*x2。之后随便来点什么，比如一个全连接层，你可以想象，当后面的层是w=[0,m,n,0], b = 0，其中，m+n=1的时候，这个模型会输出一个没有误差的结果。总共5个参数。考虑到第一层的输出有2个永远是相等的(x1*x2和x2*x1)，甚至还可以把这个也扔了，4个参数，数学意义上是没有误差的。
实际操作当中，这个的误差可以比一个10w参数的mlp还低，而且训练过程几乎光速。
这就是可能性空间。

实际上，现在所有主流的结构里面，唯一一个有x1*x2的就是注意力机制，所以可以说，注意力机制让神经网络的可能性空间在理论上是有可能增加了。
全连接层没有，conv没有，rnn看里面什么内容，或者说rnn不应该在这个地方讨论。

输入元素之间相互相乘是现在其中一个直接被用，但是从来没有验证过的例子。
还有一个是连续表达和离散表达。在电子学上叫做模拟信号和数字信号。
先稍微说一下数学上的定义。连续表达对应的是模拟信号，实数。离散有2种，一种是整数，某种意义上依然还是一种连续表达，这个在电子学上没有对应。另外一种是bool值，对应数字信号。
从数学的意义上来说，两个的转换关系也是不一样的。实数，整数，都是可以一直到无限大的，就算是密码学那一套的做法，也是循环，比如自然数的某一个位，到9了，再增加，就到0了，9和0是极端值，但是也是相邻的。bool值具有饱和性，真或真还是真，假且假还是假。
我试过用tanh或者sigmoid来模拟这种饱和性，就是当时的数字神经网络，看上去好像是ok的，但是误差传播上，两个的行为还是不同的。题外话是，最后给我搞得没办法了，搞了一套全新的目标传播体系，但是依然不行，理由在前面的”前向传播的稳定性“。

也就是说，所有应该用逻辑运算来解决的事情，用神经网络，在可能性空间的角度上来说，都是错的。
更优的做法肯定是直接设计一个行为和逻辑运算完全一致的东西，并且可以学习。题外话是，我在做。

本章的最后，提一个个人的经验，如何判断一个方法的可能性空间是否覆盖了一个特定的情况，我现在用的方法是，如果“准确性-模型尺寸”图，画出来，长得和sigmoid函数差不多，基本上可以认为是没有覆盖的。如果有覆盖，应该会出现一些模式上的突变，从而最终在准确性一类的指标上出现一个突跳。









#第四章 通用工具的力量。

最近一次被提醒这个事情是看rich sutton的采访。
所谓通用，就是不针对特定内容，或者说尽量少的人为加入先验知识。如果一个方法可以在没有某种特定的先验知识的情况下，纯粹通过和某种天然存在的环境交互，从而获取该知识，甚至是超越我们的理解的某种更优的解，那么这应该是一个非常良好的做法。
通用工具本身作为一种思维方式，不光是降低成本，更重要的是，这个ai可能还能同时形成其他很多知识。人为的给与知识可能反而是一种限制。
这个事情不光是在深度学习，强化学习，这种直接相关的领域，整个ai领域还有很多其他的工艺，都存在很强的人工干预，比如知识图谱，比如早年的自然语言处理，最早的人脸识别，都是大量的极其细致的人工专家发现的细节堆砌起来的。后来google的围棋ai，通用工具的优良能力再一次被验证。
最近的一个很严重的人工干预的例子应该主要是在数据集，和一些过于简化的强化学习训练环境。现在的直接把数据集在模型上一遍一遍的训的方法，应该是有问题的，应该是让模型自己去决定要学哪些，不学哪些，这样甚至有可能让模型能更好的抵抗训练数据里面的噪音。过于简化的训练环境里面，无论是力学的简化，还是视觉的简化，都可以认为是一种先验知识，就是我们觉得某些信息不重要，所以扔了，但是这样可能会导致一些特定的行为模式，例如通过背景的颜色的一致性来替代抠图。
强化学习里面还有一个很手动的工作，就是编写奖励函数。

我个人观点是，可能大概率是从改变奖励函数的思维方式开始吧。我现在其中一个暂停了的研究是用反向查找来替代奖励函数，但是缺点是，这个东西只能用在神经网络上，而神经网络的可能性空间的覆盖并不全。
之后所有东西改成让模型自己去探索数据集，或者直接让模型自己去从真实世界里面自己给自己整理数据集。

如果你们去看我的“人工意识”（artificial consciousness），会发现里面有一个所谓的关键输入，我现在最新的思路是，用3部分来组成，电量，机械的健康程度，和新鲜感。电量比较好理解，机械的健康程度可能需要一点技巧，但是应该也有办法解决。新鲜感可能才是重点，之前的贪婪探索是利用随机数来让模型探索更多的空间，但是如果专门做一个什么单独的模型来评判新鲜感，可能会不错吧，虽然感觉很浪费计算。本来还打算加一个“对自己未来存续概率的评估”，但是实在想不出来怎么做。









#第五章 无监督学习的数据组织方法，jepa。

虽然前面说了应该让模型自己去给自己准备数据集，但是至少在今天，无监督学习还是很重要的。
总的来说，无监督学习和有监督学习的最最核心的差异应该是成本。
有监督学习依赖标注，这个标注可能很简单，也可能很麻烦，甚至非常贵，例如有一些标注必须要通过很贵的实验才能得到。
至少我的观点不是看输入和输出是否相同。

我所知道的常见的无监督学习的数据组织方法：
echo（输出就是输入本身），降噪，升分辨率，对比学习（是否来源于同一张图片/同一段文本/同一个作者/同一个什么来源）。
如果有视频，还可以根据视频里面的像素变化来判断不同的物体，从而给对比学习提供更多的可能。

无监督学习还很容易得到模型的逆模型，虽然精度会损失。例如用model1将x转化成y，然后用model2将y转化成x。如果在auto encoder中间任何地方断开，都可以得到两个互为逆模型的子模型，当然假设是可能性空间是足够的，而且参数已经被训练得足够准确了。
类似的，无监督学习还可以让不同的可能性空间的模型进行等效化，例如用文本，经过一个固定长度的计算，得到一些固定格式的内部表达，之后用next token prediction再重新生成文本本身，让两个行为非常不相同的模型互相作为对方的逆模型。这个结构就是现在广泛使用的transformer。

除了将2个形式间实现映射（假设模型的设计和训练都没问题），还可以将更多的形式连接起来。前面提到了，降低分辨率，加噪音，也可以认为是一种形式，那么颠倒顺序，去掉其中一块，一类的各种轻度的破坏也都可以认为是一种形式，于是就有了把文本逆序（我没认证读bert，有兴趣的自己去看看），把文本/图片中间挖洞（也可以在音频上这么做吧？），作为新的形式，构建一个“破损形式n -> 共同的中间表达 -> 本来的形式”的结构，用n+1个模型将所有的状态连接起来。如果破损的形式有很强的相似/相关性，他们对应的模型也可以复用很多部分。
我没认真读jepa相关的文章，有兴趣的可以读读。
其实不光是利用破损来廉价的得到大量的相关的形式，还可以用提示词来得到。例如建立一套等效的提示词格式，在加入不同的内容之后，模型按道理应该输出一样的结果。我没仔细读，这个应该就是无监督精调，英语好像是unsupervised sft，或者写全就是，unsupervised supervised fine tuning，咳咳。

这里将一个比喻，帮助理解。前面说的，将不同的破损形式转化为共同的内部表达，最终转化为原本的完好的形式，可以尝试用一个dag（有向无环图），将整个过程画下来，然后，想象，所有的数据（包括破损之后的数据和原本的数据）都是钉子，模型是弹簧，中间那个“共同的中间表达”是一个在桌面上可以自由移动的小球，松手以后，小球会来回移动，直到到达一个最舒服的地方。训练的过程就是让它移动到最舒服的地方。
这个比喻应用到第二个，应该是，中间是几个磁铁小球，互相吸引，他们拉着弹簧，互相靠近。最后不一定会完全贴在一起。

无监督学习从根本上来说，是一种数据组织的思维方式，是将有相关性的数据放在一起，让模型去理解这种相关性的一种技术。rl里面还有一个很经典的，就是转移模型(transition model)，简单的说就是用t时刻的ob，加上action，作为输入，而t+1时刻的ob作为输出。实际上在rl的经典结构里面，各种信息还是不少的，只要是有逻辑上的相关性，就可以搭一个模型，至于这种应该算无监督还是有监督，我也不知道。
