模拟与数字。讲一些数字化的内容。模数转换里面还有一个比较门。

1个平衡。6个保护。
平衡是指所有参数的相对更新强度尽量平衡。

6个保护是：
两个传播的方向和长度保护，总共4个。牵扯一个话题，就是，方向里面保存了几乎所有信息，那么长度实际上是一个性价比很差的东西。
如果扔掉长度，那么信息就是方向里面，那么，无“有意义长度信息”的数据，信息的储存就是在方向里面，于是有了比较门。

还有2个保护，重叠参数，和零梯度（或者无梯度）。这两个是针对模型内的参数的。
还有一个关于同一个梯度被复制成多份，实际作用可能并不会被复制。

如果真的要扔长度信息，那么旋转层，比较门，两个。

实作技巧：
传参数据的规范化，或者说去掉长度信息。
离散化对方向的可能的破坏作用。
参数范围保护，去掉极端值，压回来，平移从而“平均值为0”，以及他们会破坏那些结构，不会破坏哪些。
0梯度率以及解决方案。硬选线器层为什么不如软选线器层。

标准流程：
参数是否接受梯度，平衡，保护，如果可能必须进行干堆测试，cuda，f16.

现有的结构的可训练性的分析
fcnn，cnn？，rnn（包括新的），resnet，

故障排查。
不同输入得到几乎相同的输出的时候，应该是有一个w接近0了，原因应该是相对更新强度不一致
sigmoid的输入范围太狭窄的时候，几乎不表现“非线性性”。tanh类似。

猜想。
softmax的结果不是概率
cosine similarity.
既然方向的表达能力很强，而长度很难处理，多半要丢弃，那么能不能用方向来表达大小，软one hot处理浮点数的一个猜想。
离散化，数字化，会干扰方向，那么是不是一旦进入数字化，就不应该再回模拟了。

八卦。
用纯模拟方案如何实现逻辑门，以及真的实现了吗。
自注意力（qkv风格，tfm的那个），snn里面那种自己门（qk风格）。
lstm
当年的迁移学习为什么只鼓励动最后的分类器。
resnet套resnet，tfm套tfm


意识学习：
大概的结构。







