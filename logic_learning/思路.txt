现在的一个问题是，无法做到推断时并行。（训练时并行还时比较容易想的，感觉和gpu排序那种感觉很类似）

计划是，利用gt，得到一个field，然后在这个field上增加减少，具体做法是 set集合的那种增加减少，或者用逻辑运算的方法。

总之就是得到一个固定深度的field，然后找到和gt的差，用下一个固定深度的field来调节，然后再一个。



xor的检测逻辑还太死板了，这个反正思路已经在了就无所谓了，多半只是时间问题。



然后是卡诺图上挖洞的问题。其实本来按一个顺序进行划分，不重叠的情况下，肯定有唯一解，然后拿这个唯一解来做节点，研究跳转关系应该就够了。划分顺序可以是全局一致，也可以用现在那种根据相关性，对每一个节点进行单独的指定，比如先按a分，然后a为真的时候分b，a为假的时候分c，于是在第二层，可能是b或者c，根据第一层的实际取值来决定。
刚刚想到的思路是，卡诺图因为还有无关项，所以挖洞的时候可以直接先尽量的利用无关项合并上来，但是依赖一个划分依据，就是上面那一段提到的，这个划分依据是从头到尾不能变的，这样才有办法快速的找到不重叠的最大圈，利用这个最大圈来挖洞，逻辑就简单了。至于这个划分有没有什么巧解，感觉暂时看不到。






如果可以手动写一个mlp来实现基本的门的逻辑，是否有可能用这个来组合出更复杂的结构，手动操作，不依赖反向传播。同时这个东西可以融入梯度链。
如果还是按我以前的+-1作为true和false
例如（未验算），第一层x1+x2-1, relu，当x1x2是2真，1真，0真的时候，输出分别是1，0，0，第二层只需要*2-1，只是处理一下取值范围的问题，实现了且。但是反向传播里面，因为2个值的位置是对等的，且相加，他们得到的梯度无论如何是相同的，和本身的取值无关，感觉可能不是正确的优化方向。
例如（未验算），第一层x1+x2-1, -x1-x2-1，relu，3种不同的情况可以得到的 （1,0），（0,0），（0,1），第二层权重是 1 0 1，然后*2-1，实现xor，但是梯度的问题依然和上面的分析一样。
目测和之前做的数字神经网络类似，前向传播会有突跳，应该依然是无法正常训练的。从单一一个情况来分析依然看不出优化方向，需要大量数据共同决定。